---
title: "CS 780 MMS Class Project"
subtitle: "Preliminary Report"
author: "Vega Group: Daroc Alden, Samantha Piatt, and Jeremy Walker"
date: April 12, 2018
output: pdf_document
---
```{r page_options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.asp = 0.88, fig.width = 4, error = FALSE)

source("GetDataFunctions.R")
source("ExperimentalFunctions.R")

library(knitr)
library(kableExtra)
library(randomForest)
#library(ggplot)
```
```{r load_data}
featureSubset <- c("DES_Density", "DES_Temperature_para", "DES_Temperature_perp", 
                   "DES_Velocity_x", "DES_Velocity_y", "DES_Velocity_z",
                   #"DIS_Density", "DIS_Temperature_para", "DIS_Temperature_perp", 
                   #"DIS_Velocity_x", "DIS_Velocity_y", "DIS_Velocity_z",
                   "FGM_Magnetic_Field_w", "FGM_Magnetic_Field_x", 
                   "FGM_Magnetic_Field_y", "FGM_Magnetic_Field_z")
metricSubset <- c("X", "Selected", "Priority")

baseData1 <- merge_sitl("mms_20151016.csv", "sitl_20151016.csv")
baseData2 <- merge_sitl("mms_20151204.csv", "sitl_20151204.csv")

diff.data1 <- data.frame(baseData1[,metricSubset],
                         series_metric(baseData1[,featureSubset], 30))
diff.data2 <- data.frame(baseData2[,metricSubset],
                         series_metric(baseData2[,featureSubset], 30))

shift.data1 <- data.frame(baseData1[,metricSubset],
                          shiftData(baseData1[,featureSubset]))
shift.data2 <- data.frame(baseData2[,metricSubset],
                          shiftData(baseData2[,featureSubset]))

base.data1 <- data.frame(baseData1[,metricSubset], baseData1[, featureSubset])
base.data2 <- data.frame(baseData2[,metricSubset], baseData2[, featureSubset])

diff.data <- rbind(diff.data1, diff.data1)
shift.data <- rbind(shift.data1, shift.data2)
base.data <- rbind(base.data1, base.data2)
```
```{r split_data}
# split the data twice so that there is a training(76%), test(19%), 
# and validation(5%) set.
diff.total <- split_train(diff.data, seed = 100, size = 0.95)
diff.train <- split_train(diff.total, seed = 101, size = 0.8)
diff.test <- split_test(diff.total, seed = 101, size = 0.8)
diff.validate <- split_test(diff.data, seed = 100, size = 0.95)

# Don't need a validation set with these, but we need them to be the same data 
# as the diff data for accurate comparison.
shift.total <- split_train(shift.data, seed = 100, size = 0.95)
shift.train <- split_train(shift.total, seed = 101, size = 0.8)
shift.test <- split_test(shift.total, seed = 101, size = 0.8)

base.total <- split_train(base.data, seed = 100, size = 0.95)
base.train <- split_train(base.total, seed = 101, size = 0.8)
base.test <- split_test(base.total, seed = 101, size = 0.8)
```
# Motivation

This project is intended to explore opitons for automating a key part of the ongoing Magnetospheric Multiscale Mission - the selection of which detailed datapoints ought to be downloaded from the sattelite. This job is crrently done by a Scientist in the Loop, who must spend time each day evaluating the data observed by the satelite to decide how to spend bandwidth resources.

Automating the selection of interesting data would free up valuable human time on the project. To do this, we have explored several methods for determining what data points might be interested, as detailed below.

# Related work
Accuracy of Extrapolation (Time Series) Methods: Results of a Forecasting Competition by S. Makridakit et al. is a definitive comparison of different forecasting methods on different types of and horizons of data. While this is not a forecasting problem per se, the conclusion that they draw is likely appropriate to our problem because of the complex and varied nature of the S.I.T.Ls who decide what data ought to be acquired from the satelite: "[S]tatistically sophisticated methods do not do better than simple methods ... when there is considerable randomness in the data." (pg. 142 in the Journal of Forecasting, Vol 1, Iss. No. 2)

Rolling Window Selection for Out-of-Sample Forecasting with Time-Varying Parameters by Atsushi Inoue, Lu Jin, and Barbara Rossi reccomends limiting the size of the window of data used for forecasting so that regime changes in the input data don't disrupt the fit of one's model. Unfortunatly, this is not directly applicable to our project either, since we do not have access to all the data up to the present day, but instead only have selected samples.

Time Series Prediction Using Support Vector Machines: A Survey by Jicholas Sapaankevych and Ravi Sankar points out that in the most common application for predicting time series data - the financial sector - a weighted variant of SVMs is often used called C-ascending SVMs, and that these have proven more adaptable, since they emphasize recent or current data over older data.

# Evaluation criteria

We found that the data was not identically and independantly distributed, as demonstrated by the exemplary QQ plot below.

```{r qq_plot}
m1 <- lm(Selected ~ FGM_Magnetic_Field_w + DES_Velocity_z, base.data)
qqnorm(resid(m1))
```


In fact the data is a time series. We hypothesise that adding time-delayed factors (i.e. factors which contain values from previous samples) will significantly improve the performance of many machine learning models on the validation set. See below for details.

# Methods
We compared our time series models against the basic feature set with no consideration for time, using multiple machine learning methods.

Our first time series model, which we will call time shifting, involves adding n columns to the data that contain each feature shifted n = 5 points in the past. The second time series model, which we willl call time difference, involves adding a column which takes the average differece between the current data point x and the past n = 20 data points.

## Random Forests
Using 30 splits (chosen by cross validation).
```{r rf_eval_function}
etrees <- function(fit, tr, tst, ttl, prefix = ""){
  pred <- predict(fit, newdata = tst)
  eval.test <- evaluate_data_asFrame(tst, pred)
  
  pred <- predict(fit, newdata = tr)
  eval.train <- evaluate_data_asFrame(tr, pred)
  
  pred <- predict(fit, newdata = ttl)
  eval.total <- evaluate_data_asFrame(ttl, pred)
  evals <- rbind(eval.test, eval.train, eval.total)
  row.names(evals) <- c(paste(prefix, "Test"), paste(prefix, "Train"), 
                        paste(prefix, "Total"))
  return(evals)
}
```
```{r rf_models}
depth = 30
treeCount = 1000
```

```{r}
#ttt <- rf.diff.subset
#ttt$Priority <- NULL
#blah <- rfcv(ttt, rf.diff.subset$Priority, ntree = 10)
```


```{r rf_model}
rf.diff.subset <- diff.train[ ,!(colnames(diff.train) %in% c("Selected"))]
#rf.shift.subset <- shift.train[ ,!(colnames(shift.train) %in% c("Selected"))]

# Note: mtry was picked using Cross Validation, in the commented out block above.
rf.diff.fit <- randomForest(Priority ~ . - X, data = rf.diff.subset, 
                       mtry = depth, importance = TRUE, ntree = treeCount)
```
```{r}
pred <- predict(rf.diff.fit, newdata = diff.test)
eval.test <- evaluate_data_asFrame(diff.test, pred)

rf.shift.subset <- na.omit(shift.train[ ,!(colnames(shift.train) %in% c("Selected"))])
rf.shift.fit <- randomForest(Priority ~ . - X, data = rf.shift.subset, 
                       mtry = depth, importance = TRUE, ntree = treeCount)


rf.base.subset <- base.train[ ,!(colnames(base.train) %in% c("Selected"))]
rf.base.fit <- randomForest(Priority ~ . - X, data = rf.base.subset, 
                       mtry = depth, importance = TRUE, ntree = treeCount)
```
```{r rf_evals}
rf.eval <- rbind(etrees(rf.base.fit, base.test, base.train, base.total, "Difference"),
                 etrees(rf.shift.fit, shift.test, shift.train, shift.total, "Shift"),
                 etrees(rf.diff.fit, diff.train, diff.test, diff.total, "Base"))
kable(rf.eval, "html") %>% kable_styling(bootstrap_options = "striped") %>% 
  column_spec(c(1, 5:6), bold = T)%>%
  group_rows("Basic Data", 1, 3) %>%
  group_rows("Time Shift Data", 4, 6) %>%
  group_rows("Difference Data", 7, 9)
```

## Validation

Since, during training, the version of each model using the time difference did significantly better for all models, we will compare those methods' performance on our validation set. Since none of the models perform exceptionally well here, it is likely that they have overfit. We suspect this may be an artifact of the different different SITLs on different weeks, or of our comparativly sparse training data. Regardless, the algorithm's performance on this validation set is a good indicator of its likely performance on the hidden test that you have for us, professor.

Also: Since Logistic Regression did so poorly, we've chosen to omit it here for brevity.

```{r load_validationData}
#val <- merge_sitl("mms_20151204.csv", "sitl_20151204.csv")
#val$SelectedF <- as.factor(val$Selected)
#validate <- data.frame(val[,metricSubset], series_metric(val[,featureSubset], 20))
```

### Random Forest

```{r}
pred <- predict(rf.diff.fit, newdata = diff.validate)
rf.diff.val <- evaluate_data_asFrame(diff.validate, pred)

kable(rf.diff.val, "html") %>% kable_styling(bootstrap_options = "striped") %>% column_spec(4:5, bold = T)
```

# Recommendations

TODO

# Analysis

Since Random Forests did less well on our training data, but better on our validation set, it seems likely that other methods overfit. This can be solved in a few ways, not least of which will be to acquire more data. Overfitting is evidence that our methods are too flexible - so another way to reduce their flexibility would be to use bagging or boosting in combination with another method, or to use cross-validation to choose such parameters as the amount of time-shift to use.
